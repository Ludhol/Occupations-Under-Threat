{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input and Processing\n",
    "This Jupyter Notebook contains code that takes the raw data files from Statistics Sweden and produces the files that are used in the analysis.\n",
    "\n",
    "## The data from Statistics Sweden:\n",
    "Occupational Transitions betweeen 2016 and 2017\n",
    "Seasonally adjusted vacancy and employment data between 2004 and 2019\n",
    "Seasonally and Calender adjusted unemployment data between 2004 and 2019\n",
    "Employment data by SSYK occupational code between 2016 and 2018 (THESE DATES SHOULD BE DOUBLE CHECKED)\n",
    "\n",
    "## In addition to the data from Statistics Sweden, Computerisation Probabilities from Frey & Osborne is used\n",
    "This data was developed for the american SOC (System for Occupational Classifications) and has to be translated to match Swedish data\n",
    "First the data is translated to ISCO using a key from (FINISH SENTENCE)\n",
    "Then the data is translated using a key found on Statistics Sweden's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages (check which are required)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import datetime as dt\n",
    "import community\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import statsmodels.tsa.api as tsa\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import cmocean as cmo\n",
    "\n",
    "# Write files\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occupational transitions and the Adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the occupation transition data (as well as occupation code keys) is imported\n",
    "\n",
    "data = pd.read_csv('/Users/lh/MSc_Thesis/Data_Labour/swedish_occupation_transitions.csv', sep = ';', index_col = 0)\n",
    "data.index.name = None\n",
    "data = data.drop(axis = 1, labels = 'Totalsumma')\n",
    "data = data.drop(axis = 1, index = 'Totalsumma')\n",
    "last = data.index[-1]\n",
    "data = data.rename(index={last: 'NULL'})\n",
    "\n",
    "# Drop Null and '***' columns\n",
    "data = data.iloc[0:148, 0:148]\n",
    "# ['31', '21', '11']\n",
    "\n",
    "data.drop(labels = ['31', '21', '11'], axis = 0, inplace = True)\n",
    "data.drop(labels = ['31', '21', '11'], axis = 1, inplace = True)\n",
    "\n",
    "# This section calculates the adjacency matrix A from the raw data\n",
    "\n",
    "A = pd.DataFrame(np.zeros(data.shape), columns = data.columns, index = data.index)\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    total = 0\n",
    "    for t in range(data.shape[1]):\n",
    "        if math.isnan(data.iloc[i,t]) != True:\n",
    "            total += data.iloc[i,t]\n",
    "        else:\n",
    "            data.iloc[i, t] = 0\n",
    "\n",
    "    for j in range(data.shape[1]):\n",
    "        T = data.iloc[i,j]\n",
    "        A.iloc[i,j] = (T/total)\n",
    "\n",
    "A.index = A.index.map(str)\n",
    "A.columns = A.columns.map(str)\n",
    "\n",
    "row_nonzeros = np.count_nonzero(A, axis=0)\n",
    "col_nonzeros = np.count_nonzero(A, axis=1)\n",
    "\n",
    "for i in range(len(row_nonzeros)):\n",
    "    if row_nonzeros[i] == col_nonzeros[i] & row_nonzeros[i] == 1:\n",
    "        print(A.columns[i])\n",
    "\n",
    "# SSYK 323 and 622 only has selfloops and are not connected to the main component of the graph -> should be removed\n",
    "\n",
    "data.drop(labels = ['323', '622'], axis = 0, inplace = True)\n",
    "data.drop(labels = ['323', '622'], axis = 1, inplace = True)\n",
    "\n",
    "A.drop(labels = ['323', '622'], axis = 0, inplace = True)\n",
    "A.drop(labels = ['323', '622'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frey & Osborne Computerisation Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the automation shock data from Frey and Osborne is imported and processed between occupation classification systems\n",
    "\n",
    "frey_osborne = pd.read_csv('/Users/lh/MSc_Thesis/Data_Labour/osborne_frey_data.csv', sep = ';', index_col = 0)\n",
    "\n",
    "SOC_shock = frey_osborne[['Probability', 'SOC code']]\n",
    "SOC_shock.columns = ['Computerisation Probability', 'soc10']\n",
    "\n",
    "for i in range(len(SOC_shock['soc10'])):\n",
    "    SOC_shock.iloc[i,1] = SOC_shock.iloc[i,1][0:2] + SOC_shock.iloc[i,1][3:7]\n",
    "    #SOC_shock.iloc[i,1] = SOC_shock.iloc[i,1]\n",
    "\n",
    "\n",
    "SOC_ISCO = pd.read_csv('/Users/lh/MSc_Thesis/Data_Labour/soc10_isco08.csv', sep = ',')\n",
    "for i in range(len(SOC_ISCO['isco08'])):\n",
    "    SOC_ISCO.iloc[i,1] = str(SOC_ISCO.iloc[i,1])\n",
    "    SOC_ISCO.iloc[i,0] = str(SOC_ISCO.iloc[i,0])\n",
    "    # if len(SOC_ISCO.iloc[i,1]) == 3:\n",
    "    #     SOC_ISCO.iloc[i,1] = '0' + SOC_ISCO.iloc[i,1]\n",
    "\n",
    "ISCO_SSYK = pd.read_csv('/Users/lh/MSc_Thesis/Data_Labour/nyckel_ssyk2012_isco-08.csv', sep = ';')\n",
    "ISCO_SSYK = ISCO_SSYK[['SSYK 2012 kod','ISCO-08 ']]\n",
    "ISCO_SSYK.columns = ['ssyk12', 'isco08']\n",
    "\n",
    "for i in range(len(ISCO_SSYK['isco08'])):\n",
    "    ISCO_SSYK.iloc[i,1] = str(ISCO_SSYK.iloc[i,1])\n",
    "    ISCO_SSYK.iloc[i,0] = str(ISCO_SSYK.iloc[i,0])\n",
    "\n",
    "\n",
    "# The file above contains many duplicates\n",
    "ISCO_SSYK.drop_duplicates(inplace=True)\n",
    "\n",
    "# Below transfers SOC_shock to SSYK_shock\n",
    "ISCO_shock = pd.merge(SOC_shock, SOC_ISCO, on = 'soc10')\n",
    "\n",
    "SSYK_shock = pd.merge(ISCO_shock, ISCO_SSYK, on = 'isco08')\n",
    "\n",
    "\n",
    "# The codes are 4 level need to be 3 level. Only need to change final table (SSYK_shock)\n",
    "SSYK_shock['ssyk3'] =  [code[0:3] for code in SSYK_shock['ssyk12']]\n",
    "\n",
    "SSYK3_shock = SSYK_shock[['Computerisation Probability', 'ssyk3']]\n",
    "\n",
    "\n",
    "SSYK3_shock = SSYK3_shock.groupby(['ssyk3'], as_index=False).mean()\n",
    "\n",
    "\n",
    "SSYK3 = list(SSYK3_shock['ssyk3'])\n",
    "\n",
    "G = nx.from_pandas_adjacency(A, create_using = nx.DiGraph)\n",
    "SSYK3_fromnw = list(G.nodes)\n",
    "SSYK3_fromnw = [str(node) for node in SSYK3_fromnw]\n",
    "\n",
    "SSYK_shock.to_csv('/Users/lh/MSc_Thesis/Data_Labour/occupation_shock.csv', sep = ';')\n",
    "\n",
    "# PRoblem is that certain SOC codes in osborne frey have been abbreviated with 0s. Which makes the matching miss a few rows\n",
    "# This problem can be fixed\n",
    "\n",
    "# SOC codes that are not found in the SOC-ISCO translation file\n",
    "# print(set(SOC_shock['soc10'])-set(SOC_ISCO['soc10']))\n",
    "# {'292037', '292055', '499799', '291060', '394831', '319799', '292799', '251000', '253999', '151179', '474799', '131078', '452090', '299799', '151150', '151799', '519399', '291111'}\n",
    "\n",
    "# Focus on '251000', '151150', '291060'\n",
    "# 291060 solves 221 because 291060 doesnt exist in soc_isco\n",
    "# 291141, 291151, 291171, 291161 <- 222\n",
    "# 29-1111 is not used anymore, 29-1141 should be used instead: https://www.onetonline.org/find/quick?s=29-1111\n",
    "\n",
    "# 231 ssyk: soc_isco översätter till isco 2310 som inte existerar i isco_ssyk nykeln där det istället är 231X. Bör alltså gå att lösa\n",
    "# 251000: Post-secondary teachers is translated as 2310 SSYK\n",
    "\n",
    "# SSYK codes not found in ISCO-SSYK translation file\n",
    "# print(list(set(SSYK3_fromnw) - set(SSYK3)))\n",
    "# ['221', '21', '11', '222', '231', '31']\n",
    "# ['21', '11', '31'] are military occupations and we do not have computersiation probabilities for these\n",
    "\n",
    "# focus on ['221', '222', '231']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deviations from potential GDP from Konjunktur Institutet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_gap = pd.read_csv('/Users/lh/MSc_Thesis/Data_Labour/bnp-gap.csv', sep = ';')\n",
    "gdp_gap['Qtr'] = pd.to_datetime(gdp_gap.date).dt.quarter\n",
    "gdp_gap['date'] = [gdp_gap['date'].iloc[i][0:4] + 'Q' + str(gdp_gap['Qtr'].iloc[i]) for i in range(len(gdp_gap['date']))]\n",
    "gdp_gap['recession'] = [1 if gap <= 0 else 0 for gap in list(gdp_gap['BNP-gap'])]\n",
    "gdp_gap.rename(columns = {'BNP-gap': 'gdp_gap'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from Statistics Sweden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for calibrating and setting up the model\n",
    "employment_SSYK = pd.read_csv('/Users/lh/MSc_Thesis/Data_Labour/employment_SSYK.csv', sep = ',')\n",
    "employment_SSYK.rename(columns = {'Yrke (SSYK 2012)':'SSYK'}, inplace = True)\n",
    "# SSYK code and years as columns\n",
    "\n",
    "# New calibration data - seasonally adjusted\n",
    "employment_sa = pd.read_csv('/Users/lh/MSc_Thesis/Data_Labour/employment_quarterly.csv', sep = ';')\n",
    "employment_sa['date'] = [str(2000 + employment_sa['year'].iloc[i])+'Q'+str(employment_sa['quarter'].iloc[i]) for i in range(len(employment_sa))]\n",
    "employment_sa = employment_sa[['date', 'e_sa', 'e_trend']]\n",
    "employment_sa['e_sa'] = [int(float(string.replace(',','.'))*1000) for string in employment_sa['e_sa']]\n",
    "employment_sa['e_trend'] = [int(float(string.replace(',','.'))*1000) for string in employment_sa['e_trend']]\n",
    "\n",
    "\n",
    "unemployment_all = pd.read_csv('/Users/lh/MSc_Thesis/Data_Labour/unemployment_quarterly.csv', sep = ';')\n",
    "unemployment_all['date'] = [str(2000 + unemployment_all['year'].iloc[i])+'Q'+str(unemployment_all['quarter'].iloc[i]) for i in range(len(unemployment_all))]\n",
    "unemployment_sa = unemployment_all[['date', 'u_sa', 'u_trend']]\n",
    "\n",
    "unemployment_sa['u_sa'] = [float(string.replace(',','.')) for string in unemployment_sa['u_sa']]\n",
    "unemployment_sa['u_trend'] = [float(string.replace(',','.')) for string in unemployment_sa['u_trend']]\n",
    "\n",
    "vac_rate_all = pd.read_csv('/Users/lh/MSc_Thesis/Data_Labour/Vacancy Data/sa_2004-2019.csv', sep = ';')\n",
    "\n",
    "sa_calibration_data = pd.merge(unemployment_sa, vac_rate_all[['date', 'sa_vac', 'na_vac']], on ='date')\n",
    "sa_calibration_data = pd.merge(sa_calibration_data, employment_sa, on = 'date')\n",
    "sa_calibration_data = pd.merge(sa_calibration_data, gdp_gap[['date', 'recession']], on = 'date')\n",
    "\n",
    "sa_calibration_data['sa_vac_rate'] = sa_calibration_data['sa_vac']*100/(sa_calibration_data['e_trend']+sa_calibration_data['sa_vac'])\n"
   ]
  }
 ]
}