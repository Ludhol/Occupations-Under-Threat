{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input and Processing\n",
    "This Jupyter Notebook contains code that takes the raw data files from Statistics Sweden and produces the files that are used in the analysis.\n",
    "\n",
    "## The data from Statistics Sweden:\n",
    "Occupational Transitions betweeen 2016 and 2017\n",
    "Quarterly Seasonally adjusted vacancy and employment data between 2004 and 2019\n",
    "Quarterly Seasonally and Calender adjusted unemployment data between 2004 and 2019\n",
    "Yearly Employment data distributed by SSYK occupational code between 2014 and 2018\n",
    "Yearly averages of total weekly hours worked between 2014 and 2018\n",
    "\n",
    "## Computerisation Probabilities from Frey & Osborne\n",
    "This data was developed for the american SOC (System for Occupational Classifications) and has to be translated to match Swedish data\n",
    "First the data is translated to ISCO using a key from https://ibs.org.pl/en/resources/occupation-classifications-crosswalks-from-onet-soc-to-isco/ which is based on work of David Autor and Daron Acemoglu: http://economics.mit.edu/faculty/dautor/data/acemoglu\n",
    "Then the data is translated using a key found on Statistics Sweden's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages (check which are required)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import datetime as dt\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import cmocean as cmo\n",
    "\n",
    "# Write files\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occupational transitions and the Adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323\n",
      "622\n"
     ]
    }
   ],
   "source": [
    "# This is where the occupation transition data (as well as occupation code keys) is imported\n",
    "\n",
    "data = pd.read_csv('../Data_Labour/swedish_occupation_transitions.csv', sep = ';', index_col = 0)\n",
    "data.index.name = None\n",
    "data = data.drop(axis = 1, labels = 'Totalsumma')\n",
    "data = data.drop(axis = 1, index = 'Totalsumma')\n",
    "last = data.index[-1]\n",
    "data = data.rename(index={last: 'NULL'})\n",
    "\n",
    "# Drop Null and '***' columns\n",
    "data = data.iloc[0:148, 0:148]\n",
    "\n",
    "# ['31', '21', '11'] Are actually meant to be '031', '021' and '011'. \n",
    "# The 0 denotes that these are military occupations. \n",
    "# The data from Frey and Osborne do not cover military information which means that we cannot include \n",
    "# it in our analysis\n",
    "data.drop(labels = ['31', '21', '11'], axis = 0, inplace = True)\n",
    "data.drop(labels = ['31', '21', '11'], axis = 1, inplace = True)\n",
    "\n",
    "# This section calculates the adjacency matrix A from the raw data\n",
    "A = pd.DataFrame(np.zeros(data.shape), columns = data.columns, index = data.index)\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    total = 0\n",
    "    for t in range(data.shape[1]):\n",
    "        if math.isnan(data.iloc[i,t]) != True:\n",
    "            total += data.iloc[i,t]\n",
    "        else:\n",
    "            data.iloc[i, t] = 0\n",
    "\n",
    "    for j in range(data.shape[1]):\n",
    "        T = data.iloc[i,j]\n",
    "        A.iloc[i,j] = (T/total)\n",
    "\n",
    "A.index = A.index.map(str)\n",
    "A.columns = A.columns.map(str)\n",
    "\n",
    "row_nonzeros = np.count_nonzero(A, axis=0)\n",
    "col_nonzeros = np.count_nonzero(A, axis=1)\n",
    "\n",
    "for i in range(len(row_nonzeros)):\n",
    "    if row_nonzeros[i] == col_nonzeros[i] & row_nonzeros[i] == 1:\n",
    "        print(A.columns[i])\n",
    "\n",
    "# SSYK 323 and 622 only has selfloops and are not connected to the main component of the graph \n",
    "# As discussed in the thesis - these are removed\n",
    "# data.drop(labels = ['323', '622'], axis = 0, inplace = True)\n",
    "# data.drop(labels = ['323', '622'], axis = 1, inplace = True)\n",
    "# A.drop(labels = ['323', '622'], axis = 0, inplace = True)\n",
    "# A.drop(labels = ['323', '622'], axis = 1, inplace = True)\n",
    "\n",
    "G = nx.from_pandas_adjacency(A, create_using = nx.DiGraph)\n",
    "print('The code outputs node labels which have no edges')\n",
    "\n",
    "data.to_csv('../Data_Labour/Occupation_transitions.csv', sep = ',')\n",
    "nx.write_graphml(G, '../Data_Labour/Occ_mob_sweden.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frey & Osborne Computerisation Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the automation shock data from Frey and Osborne is imported and processed between occupation classification systems\n",
    "\n",
    "frey_osborne = pd.read_csv('../Data_Labour/osborne_frey_data.csv', sep = ';', index_col = 0)\n",
    "\n",
    "SOC_shock = frey_osborne[['Probability', 'SOC code']]\n",
    "SOC_shock.columns = ['Computerisation Probability', 'soc10']\n",
    "\n",
    "for i in range(len(SOC_shock['soc10'])):\n",
    "    SOC_shock.iloc[i,1] = SOC_shock.iloc[i,1][0:2] + SOC_shock.iloc[i,1][3:7]\n",
    "    #SOC_shock.iloc[i,1] = SOC_shock.iloc[i,1]\n",
    "\n",
    "\n",
    "SOC_ISCO = pd.read_csv('../Data_Labour/soc10_isco08.csv', sep = ',')\n",
    "for i in range(len(SOC_ISCO['isco08'])):\n",
    "    SOC_ISCO.iloc[i,1] = str(SOC_ISCO.iloc[i,1])\n",
    "    SOC_ISCO.iloc[i,0] = str(SOC_ISCO.iloc[i,0])\n",
    "    # if len(SOC_ISCO.iloc[i,1]) == 3:\n",
    "    #     SOC_ISCO.iloc[i,1] = '0' + SOC_ISCO.iloc[i,1]\n",
    "\n",
    "ISCO_SSYK = pd.read_csv('../Data_Labour/nyckel_ssyk2012_isco-08.csv', sep = ';')\n",
    "ISCO_SSYK = ISCO_SSYK[['SSYK 2012 kod','ISCO-08 ']]\n",
    "ISCO_SSYK.columns = ['ssyk12', 'isco08']\n",
    "\n",
    "for i in range(len(ISCO_SSYK['isco08'])):\n",
    "    ISCO_SSYK.iloc[i,1] = str(ISCO_SSYK.iloc[i,1])\n",
    "    ISCO_SSYK.iloc[i,0] = str(ISCO_SSYK.iloc[i,0])\n",
    "\n",
    "\n",
    "# The file above contains many duplicates\n",
    "ISCO_SSYK.drop_duplicates(inplace=True)\n",
    "\n",
    "# Below transfers SOC_shock to SSYK_shock\n",
    "ISCO_shock = pd.merge(SOC_shock, SOC_ISCO, on = 'soc10')\n",
    "\n",
    "SSYK_shock = pd.merge(ISCO_shock, ISCO_SSYK, on = 'isco08')\n",
    "\n",
    "\n",
    "# The codes are 4 level need to be 3 level. Only need to change final table (SSYK_shock)\n",
    "SSYK_shock['ssyk3'] =  [str(code[0:3]) for code in SSYK_shock['ssyk12']]\n",
    "\n",
    "SSYK3_shock = SSYK_shock[['Computerisation Probability', 'ssyk3']]\n",
    "\n",
    "SSYK3 = list(SSYK3_shock['ssyk3'])\n",
    "\n",
    "SSYK3_shock = SSYK3_shock.groupby(['ssyk3']).mean()\n",
    "#SSYK3_shock['ssyk3'] =  [str(code) for code in SSYK3_shock['ssyk3']]\n",
    "\n",
    "G = nx.from_pandas_adjacency(A, create_using = nx.DiGraph)\n",
    "SSYK3_fromnw = list(G.nodes)\n",
    "SSYK3_fromnw = [str(node) for node in SSYK3_fromnw]\n",
    "\n",
    "SSYK3_shock.to_csv('../Data_Labour/occupation_shock.csv', sep = ',')\n",
    "\n",
    "# Problem is that certain SOC codes in osborne frey have been abbreviated with 0s. \n",
    "# Which makes the matching miss a few rows\n",
    "# This problem can be fixed\n",
    "\n",
    "# SOC codes that are not found in the SOC-ISCO translation file\n",
    "# print(set(SOC_shock['soc10'])-set(SOC_ISCO['soc10']))\n",
    "# {'292037', '292055', '499799', '291060', '394831', '319799', '292799', '251000', '253999', \n",
    "# '151179', '474799', '131078', '452090', '299799', '151150', '151799', '519399', '291111'}\n",
    "\n",
    "# Focus on '251000', '151150', '291060'\n",
    "# 291060 solves 221 because 291060 doesnt exist in soc_isco\n",
    "# 291141, 291151, 291171, 291161 <- 222\n",
    "# 29-1111 is not used anymore, 29-1141 should be used instead: https://www.onetonline.org/find/quick?s=29-1111\n",
    "\n",
    "# 231 ssyk: soc_isco översätter till isco 2310 som inte existerar i isco_ssyk nykeln där det istället är 231X. \n",
    "\n",
    "# 251000: Post-secondary teachers is translated as 2310 SSYK\n",
    "\n",
    "# SSYK codes not found in ISCO-SSYK translation file\n",
    "# print(list(set(SSYK3_fromnw) - set(SSYK3)))\n",
    "# ['221', '21', '11', '222', '231', '31']\n",
    "# ['21', '11', '31'] are military occupations and we do not have computersiation probabilities for these\n",
    "\n",
    "# focus on ['221', '222', '231']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occupational employment distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSYK_labels = pd.read_csv('../Data_Labour/Ssyk-2012-koder.csv', sep = ';').astype(str)\n",
    "\n",
    "SSYK3_shock = SSYK3_shock.merge(SSYK_labels, how = 'left', on = 'ssyk3')\n",
    "\n",
    "employment_SSYK = pd.read_csv('../Data_Labour/employment_SSYK.csv', sep = ',').astype(str)\n",
    "employment_SSYK.rename(columns = {'Yrke (SSYK 2012)':'ssyk3'}, inplace = True)\n",
    "employment_SSYK.to_csv('../Data_Labour/occupational_employment.csv', sep = ',')\n",
    "\n",
    "occupational_data = SSYK3_shock.merge(employment_SSYK, on = 'ssyk3')\n",
    "occupational_data.to_csv('../Data_Labour/occupational_data.csv', sep = ',')\n",
    "\n",
    "# SSYK code and years as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssyk3                                                  622\n",
       "Computerisation Probability                           0.72\n",
       "swedish                             Fiskodlare och fiskare\n",
       "english                        Fishery workers and fishers\n",
       "2014                                                   474\n",
       "2015                                                   489\n",
       "2016                                                   553\n",
       "2017                                                   586\n",
       "2018                                                   604\n",
       "Name: 622, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupational_data.index = occupational_data.ssyk3\n",
    "occupational_data.loc['622']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Data used to calculate the post shock demand\n",
    "    comp_prob = nx.get_node_attributes(G, 'comp_prob')\n",
    "    average_hours_worked_0 = avg_hours_0\n",
    "\n",
    "    vacancies = nx.get_node_attributes(G, 'vacancies')\n",
    "    employed = nx.get_node_attributes(G, 'employed')\n",
    "\n",
    "    # e_0 = {key:val for key, val in employed.items()}\n",
    "    demand_0 = {occ:len(vacancies[occ]) + employed[occ] for occ in vacancies.keys()} \n",
    "\n",
    "    # Calculate the post shock demand for each occupation\n",
    "    L = sum(demand_0.values())\n",
    "    final_hours_worked = {occ : average_hours_worked_0*employed[occ]*(1-prob) for occ, prob in comp_prob.items()}\n",
    "    final_average_hours_worked = sum(final_hours_worked.values())/L\n",
    "\n",
    "    # Post shock demand\n",
    "    final_demand = {occupation: round(hours/final_average_hours_worked) for occupation, hours in final_hours_worked.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deviations from potential GDP from Konjunktur Institutet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_gap = pd.read_csv('../Data_Labour/bnp-gap.csv', sep = ';')\n",
    "gdp_gap['Qtr'] = pd.to_datetime(gdp_gap.date).dt.quarter\n",
    "gdp_gap['date'] = [gdp_gap['date'].iloc[i][0:4] + 'Q' + str(gdp_gap['Qtr'].iloc[i]) for i in range(len(gdp_gap['date']))]\n",
    "gdp_gap['recession'] = [1 if gap <= 0 else 0 for gap in list(gdp_gap['BNP-gap'])]\n",
    "gap_offset = zip(gdp_gap['BNP-gap'].iloc[1:],gdp_gap['BNP-gap'].iloc[:-1])\n",
    "change_ls = [(gap_t1 - gap_t0)/gap_t0 for gap_t1, gap_t0 in gap_offset]\n",
    "change_ls.insert(0,float('NaN'))\n",
    "gdp_gap['gap_change'] = change_ls\n",
    "gdp_gap.rename(columns = {'BNP-gap': 'gdp_gap'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from Statistics Sweden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New calibration data - seasonally adjusted\n",
    "employment_sa = pd.read_csv('../Data_Labour/employment_quarterly.csv', sep = ';')\n",
    "employment_sa['date'] = [str(2000 + employment_sa['year'].iloc[i])+'Q'+str(employment_sa['quarter'].iloc[i]) for i in range(len(employment_sa))]\n",
    "employment_sa = employment_sa[['date', 'e_sa', 'e_trend']]\n",
    "employment_sa['e_sa'] = [int(float(string.replace(',','.'))*1000) for string in employment_sa['e_sa']]\n",
    "employment_sa['e_trend'] = [int(float(string.replace(',','.'))*1000) for string in employment_sa['e_trend']]\n",
    "\n",
    "\n",
    "unemployment_all = pd.read_csv('../Data_Labour/unemployment_quarterly.csv', sep = ';')\n",
    "unemployment_all['date'] = [str(2000 + unemployment_all['year'].iloc[i])+'Q'+str(unemployment_all['quarter'].iloc[i]) for i in range(len(unemployment_all))]\n",
    "unemployment_sa = unemployment_all[['date', 'u_sa', 'u_trend']]\n",
    "\n",
    "unemployment_sa['u_sa'] = [float(string.replace(',','.')) for string in unemployment_sa['u_sa']]\n",
    "unemployment_sa['u_trend'] = [float(string.replace(',','.')) for string in unemployment_sa['u_trend']]\n",
    "\n",
    "vac_rate_all = pd.read_csv('../Data_Labour/Vacancy Data/sa_2004-2019.csv', sep = ';')\n",
    "\n",
    "sa_calibration_data = pd.merge(unemployment_sa, vac_rate_all[['date', 'sa_vac', 'na_vac']], on ='date')\n",
    "sa_calibration_data = pd.merge(sa_calibration_data, employment_sa, on = 'date')\n",
    "sa_calibration_data = pd.merge(sa_calibration_data, gdp_gap[['date', 'recession', 'gap_change']], on = 'date')\n",
    "\n",
    "sa_calibration_data['sa_vac_rate'] = sa_calibration_data['sa_vac']*100/(sa_calibration_data['e_trend']+sa_calibration_data['sa_vac'])\n",
    "sa_calibration_data['year'] = [date[:4] for date in sa_calibration_data['date']]\n",
    "\n",
    "sa_calibration_data['workforce'] = sa_calibration_data['e_trend']/(1-sa_calibration_data['u_trend']/100)\n",
    "\n",
    "sa_calibration_data.to_csv('../Data_Labour/calibration_data.csv', sep = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u_trend    6.34\n",
       "Name: 2018, dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hours worked data (from Statistics Sweden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_worked = pd.read_csv('../Data_Labour/hours_worked_sa.csv', sep =',')\n",
    "hours_worked.drop(labels = ['ekonomisk indikator'], axis = 1, inplace = True)\n",
    "hours_worked = hours_worked.transpose()\n",
    "hours_worked.columns = ['hours/week']\n",
    "hours_worked['hours/week'] = hours_worked['hours/week']*1000\n",
    "hours_worked['year'] = [date[:4] for date in hours_worked.index]\n",
    "yearly_hours_worked = hours_worked.groupby(['year']).mean()\n",
    "\n",
    "yearly_employment = sa_calibration_data[['year','e_trend']].groupby(['year']).mean()\n",
    "yearly_hours_employment = pd.merge(yearly_employment, yearly_hours_worked, on = 'year')\n",
    "yearly_hours_employment['average_hours/week'] = yearly_hours_employment['hours/week']/yearly_hours_employment['e_trend']\n",
    "yearly_hours_employment['average_hours/year'] = yearly_hours_employment['average_hours/week']*52\n",
    "\n",
    "yearly_hours_employment.to_csv('../Data_Labour/hours_data.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing simulation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the simulated data\n",
    "vac_data = pd.read_csv('../Data_Labour/vac_simulation.csv', sep = ',')\n",
    "emp_data = pd.read_csv('../Data_Labour/emp_simulation.csv', sep = ',')\n",
    "unemp_data = pd.read_csv('../Data_Labour/unemp_simulation.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bitd863a529c1fa4d128753e3d722b9a701"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
