{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook takes the data and conducts the Automation Shock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from data_processing.ipynb\n",
    "sa_calibration_data = pd.read_csv('../Data_Labour/calibration_data.csv')\n",
    "employment_SSYK = pd.read_csv('../Data_Labour/occupational_employment.csv', sep = ',')\n",
    "SSYK_shock = pd.read_csv('../Data_Labour/occupation_shock.csv', sep = ',')\n",
    "\n",
    "G = nx.read_graphml('../Data_Labour/Occ_mob_sweden.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_func1(individual):\n",
    "    while individual[1] < individual[2]:\n",
    "        individual[1] += 0.001\n",
    "    employment = employment_SSYK[['SSYK', '2018']]\n",
    "    employment = {str(employment['SSYK'].iloc[i]):employment['2018'].iloc[i] for i in range(len(employment))}\n",
    "    node_names = G.nodes()\n",
    "\n",
    "    # setup network\n",
    "    employed = {str(name):e for name,e in employment.items() if str(name) in node_names}\n",
    "    unemployed = {name:0 for name in node_names}\n",
    "    vacancies = {name:0 for name in node_names}\n",
    "    applications = {name:[] for name in node_names}\n",
    "    target_demand = {str(name):e for name,e in employment.items() if str(name) in node_names}\n",
    "    of_data = SSYK_shock.groupby(by = ['ssyk3'], axis = 0).mean()\n",
    "    of_data = of_data.to_dict()['Computerisation Probability']\n",
    "    attributes = {'employed':employed, 'unemployed':unemployed, 'vacancies':vacancies, 'applications':applications,\n",
    "                    'target_demand':target_demand, 'comp_prob':of_data}\n",
    "\n",
    "    # Save parameters\n",
    "\n",
    "\n",
    "    # Calibration data\n",
    "    start = 18\n",
    "    end = 59\n",
    "    empirical_data = sa_calibration_data.iloc[start:end]\n",
    "    parameters = {}\n",
    "    \n",
    "    parameters['T'] = 10.25\n",
    "    # Shock parameters (not there yet)\n",
    "    parameters['t_0'] = 10\n",
    "    parameters['k'] = 1\n",
    "    parameters['L'] = 1\n",
    "    parameters['avg_hours_0'] = 1\n",
    "    parameters['shock_start'] = 1\n",
    "    parameters['timestep'] = 6\n",
    "\n",
    "    calibration_output = 'evo'\n",
    "\n",
    "    # Run simulation\n",
    "    fitness = deterministic_simulation(individual, G, parameters['T'], empirical_data,\n",
    "                    parameters['k'], parameters['avg_hours_0'], parameters['shock_start'],\n",
    "                    attributes, calibration_output, False)\n",
    "    print('a:', individual[0], 'delta_u:', individual[1], 'delta_nu:', individual[2], 'gamma_u:', individual[3], 'timestep:', individual[4], 'fitness:', fitness)\n",
    "\n",
    "    #string = 'a: ' + str(individual[0]) + ' delta_u: ' + str(individual[1]) + ' delta_nu: ', str(individual[2]) + ' gamma_u: ' + str(individual[3]) + ' fitness: ' + str(fitness)\n",
    "    \n",
    "    #f = open(\"../Data_Labour/parameters_fitness.txt\",\"a\")\n",
    "    #f.write(string + \"\\n\")\n",
    "    #f.close\n",
    "    #results.append({'fitness':fitness, 'individual':individual})\n",
    "\n",
    "    return fitness,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_simulation(individual, G, period, empirical_data, k, avg_hours_0, shock_start, attributes, \n",
    "                             calibration_output = False, steady_state = False):\n",
    "\n",
    "    #set_attributes(G, data)\n",
    "    # This needs to be put into the network (used as starting point)\n",
    "    # 0.010277616112863805 delta_nu: 0.0018934089531946136 gamma_u: 0.12611162194060122\n",
    "\n",
    "    a = individual[0]\n",
    "    delta_u = individual[1]\n",
    "    delta_nu = individual[2]\n",
    "    gamma_u = individual[3]\n",
    "    timestep = individual[4]\n",
    "\n",
    "    for key, value in attributes.items():\n",
    "        nx.set_node_attributes(G, value, str(key))\n",
    "    \n",
    "    if steady_state == False:\n",
    "        timesteps = int(period*52/timestep)*3 # Total amount of steps\n",
    "        print(timesteps)\n",
    "    else:\n",
    "        timesteps = round(period*52/timestep)\n",
    "    T_steps = int(period*52/timestep) # Steps during simulated business cycle\n",
    "    \n",
    "    vacancies = nx.get_node_attributes(G, 'vacancies')\n",
    "    employed = nx.get_node_attributes(G, 'employed')\n",
    "\n",
    "    demand_0 = {}\n",
    "\n",
    "    for key in vacancies.keys():\n",
    "        demand_0[key] = vacancies[key] + employed[key] \n",
    "\n",
    "    vac_data = []\n",
    "    emp_data = []\n",
    "    unemp_data = []\n",
    "    td_data = []\n",
    "\n",
    "    # Variables to calculate the post shock demand\n",
    "    risk_factor = nx.get_node_attributes(G, 'comp_prob')\n",
    "    average_hours_worked_0 = avg_hours_0\n",
    "    L = sum(demand_0.values())\n",
    "    \n",
    "\n",
    "    final_hours_worked = {}\n",
    "\n",
    "    for occupation in risk_factor.keys():\n",
    "        final_hours_worked[occupation] = average_hours_worked_0*employed[occupation]*(1-risk_factor[occupation])\n",
    "\n",
    "    final_average_hours_worked = sum(final_hours_worked.values())/L\n",
    "\n",
    "    # Post shock demand\n",
    "    final_demand = {occupation:hours/final_average_hours_worked for occupation, hours in final_hours_worked.items()}\n",
    "\n",
    "    occupations = G.nodes()\n",
    "    time = dt.datetime.now()\n",
    "    \n",
    "\n",
    "    print('Simulation started at: ', time)\n",
    "    for t in range(timesteps):\n",
    "        ny = nx.get_node_attributes(G, 'vacancies')\n",
    "        u = nx.get_node_attributes(G, 'unemployed')\n",
    "        e = nx.get_node_attributes(G, 'employed')\n",
    "        A = nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "        s = {}\n",
    "        f = {}\n",
    "        for j in occupations:\n",
    "            s[j] = []\n",
    "            for i in G.predecessors(j):\n",
    "                ny_A_sum = sum([ny[k]*A[(i,k)] for k in G.neighbors(i)])\n",
    "                if ny_A_sum == 0:\n",
    "                    s[j].append(0)\n",
    "                else:\n",
    "                    s[j].append(u[i]*ny[j]*A[(i,j)]/ny_A_sum)\n",
    "\n",
    "            s[j] = sum(s[j])\n",
    "            for i in G.predecessors(j):\n",
    "                ny_A_sum = sum([ny[k]*A[(i,k)] for k in G.neighbors(i)])\n",
    "                if s[j]*ny_A_sum == 0:\n",
    "                    f[(i,j)] = 0\n",
    "                else:\n",
    "                    f[(i,j)] = u[i]*(ny[j]**(2))*A[(i,j)]*(1 - math.exp(-s[j]/ny[j]))/(s[j]*ny_A_sum)\n",
    "\n",
    "        new_e = {}\n",
    "        new_u = {}\n",
    "        new_ny = {}\n",
    "        \n",
    "        target_demand = nx.get_node_attributes(G, 'target_demand')\n",
    "        current_demand = {}\n",
    "\n",
    "        for i in occupations:\n",
    "            current_demand[i] = ny[i] + e[i]\n",
    "            demand_diff = max(0, current_demand[i] - target_demand[i])\n",
    "\n",
    "            f_i = sum([f[(j,i)] for j in G.predecessors(i)])\n",
    "\n",
    "            new_e[i] = e[i] - delta_u*e[i] + (1 - delta_u)*gamma_u*demand_diff + f_i\n",
    "\n",
    "            f_j = sum([f[(i,j)] for j in G.successors(i)])\n",
    "\n",
    "            new_u[i] = u[i] + delta_u*e[i] + (1 - delta_u)*gamma_u*demand_diff - f_j\n",
    "\n",
    "            demand_diff = max(0, target_demand[i]-current_demand[i])\n",
    "\n",
    "            new_ny[i] = ny[i] + delta_nu*e[i] + (1-delta_nu)*gamma_u*demand_diff - f_i\n",
    "\n",
    "        nx.set_node_attributes(G, new_ny, 'vacancies')\n",
    "        nx.set_node_attributes(G, new_e, 'employed')\n",
    "        nx.set_node_attributes(G, new_u, 'unemployed')\n",
    "\n",
    "        vac_data.append(nx.get_node_attributes(G, 'vacancies'))\n",
    "        unemp_data.append(nx.get_node_attributes(G, 'unemployed'))\n",
    "        emp_data.append(nx.get_node_attributes(G, 'employed'))\n",
    "        td_data.append(target_demand)\n",
    "        \n",
    "        #if shock_start < t:\n",
    "        #    DDOM.shock(G, demand_0, final_demand, t, t_0, k)\n",
    "\n",
    "        if T_steps < t:\n",
    "            DDOM.update_target_demand(G, demand_0, t, T_steps, a)\n",
    "        # order should be checked and changed\n",
    "        # if t > shock_start:\n",
    "        #    shock(G, demand_0, final_demand, t, t_0, k)\n",
    "\n",
    "    model_data = {'vacancies': vac_data, 'unemployment': unemp_data, 'employment':emp_data}\n",
    "    # Empirical data\n",
    "    e_vac_rate = empirical_data['sa_vac_rate']\n",
    "    e_unemployed = empirical_data['u_trend']\n",
    "\n",
    "    e_seq = [(u, e_vac_rate.iloc[i]) for i, u in enumerate(e_unemployed)]#\n",
    "    e_u_max = max(e_unemployed)\n",
    "    e_u_min = min(e_unemployed)\n",
    "    e_vac_max = max(e_vac_rate)\n",
    "    e_vac_min = min(e_vac_rate)\n",
    "\n",
    "    e_u_mean = np.mean(e_unemployed)\n",
    "    e_vac_mean = np.mean(e_vac_rate)\n",
    "\n",
    "    A_e = Polygon(e_seq).buffer(0)\n",
    "\n",
    "    # Cost is a vector of deviations from goal. Should be 0\n",
    "    cost = DDOM.calibration_calculation(empirical_data, model_data, A_e, T_steps)\n",
    "\n",
    "    cost['A_e'] = A_e.area\n",
    "    if steady_state == False:\n",
    "        if type(cost['cost']) != str:\n",
    "            fitness = cost['cost']\n",
    "        else:\n",
    "            fitness = [\n",
    "            abs(e_u_max - cost['m_u_max']),\n",
    "            abs(e_u_min - cost['m_u_min']),\n",
    "            abs(e_vac_max - cost['m_vac_max']),\n",
    "            abs(e_vac_min - cost['m_vac_min'])\n",
    "            ]\n",
    "            fitness = np.linalg.norm(fitness)\n",
    "    else:\n",
    "        fitness = [abs(e_u_mean - cost['m_u_ss']), abs(e_vac_mean - cost['m_vac_ss'])]\n",
    "        fitness = fitness[0]*0.3 + fitness[1]*0.7\n",
    "    \n",
    "    time = dt.datetime.now()- time\n",
    "    print('Simulation took: ', time)\n",
    "    \n",
    "    if calibration_output == 'evo':\n",
    "        return fitness\n",
    "    elif calibration_output == 'True':\n",
    "        cost['time'] = time\n",
    "        return cost\n",
    "    else:\n",
    "        vac_data = pd.DataFrame(vac_data)\n",
    "        unemp_data = pd.DataFrame(unemp_data)\n",
    "        emp_data = pd.DataFrame(emp_data)\n",
    "        return {'vacancy_data': vac_data, 'unemployment_data': unemp_data, 'employment_data': emp_data, 'cost': cost}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}